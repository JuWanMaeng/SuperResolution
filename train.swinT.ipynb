{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joowan/anaconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # specify path to data\n",
    "# path2data = 'dataset/STL'\n",
    "\n",
    "# # if not exists the path, make the directory\n",
    "# if not os.path.exists(path2data):\n",
    "#     os.mkdir(path2data)\n",
    "\n",
    "# # load dataset\n",
    "# train_ds = datasets.STL10(path2data, split='train', download=True, transform=transforms.ToTensor())\n",
    "# val_ds = datasets.STL10(path2data, split='test', download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# print(len(train_ds))\n",
    "# print(len(val_ds))\n",
    "\n",
    "\n",
    "# # define transformation\n",
    "# transformation = transforms.Compose([\n",
    "#                     transforms.ToTensor(),\n",
    "#                     transforms.Resize(224),\n",
    "#                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "# # apply transformation to dataset\n",
    "# train_ds.transform = transformation\n",
    "# val_ds.transform = transformation\n",
    "\n",
    "# # make dataloade\n",
    "# train_dl = DataLoader(train_ds, batch_size=64, shuffle=True,num_workers=8)\n",
    "# val_dl = DataLoader(val_ds, batch_size=16, shuffle=True,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "path2data = 'dataset/STL'\n",
    "\n",
    "# load dataset\n",
    "train_ds = datasets.STL10(path2data, split='train', download=False, transform=transforms.ToTensor())\n",
    "val_ds = datasets.STL10(path2data, split='test', download=False, transform=transforms.ToTensor())\n",
    "\n",
    "print(len(train_ds))\n",
    "print(len(val_ds))\n",
    "\n",
    "\n",
    "# define transformation\n",
    "transformation = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize(224),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# apply transformation to dataset\n",
    "train_ds.transform = transformation\n",
    "val_ds.transform = transformation\n",
    "\n",
    "# make dataloade\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True,num_workers=8)\n",
    "val_dl = DataLoader(val_ds, batch_size=16, shuffle=True,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset STL10\n",
       "    Number of datapoints: 8000\n",
       "    Root location: dataset/STL\n",
       "    Split: test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Fused window process have not been installed. Please refer to get_started.md for installation.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from models import swin_transformer\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device='cuda:0'\n",
    "net=swin_transformer.SwinTransformer(num_classes=10)\n",
    "net=net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: |                                                 | 0/79 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "train: |▌                                        | 1/79 [00:00<01:07,  1.15it/s]\n",
      "train: |█                                        | 2/79 [00:01<00:42,  1.83it/s]\n",
      "train: |█▌                                       | 3/79 [00:01<00:34,  2.20it/s]\n",
      "train: |██                                       | 4/79 [00:01<00:29,  2.51it/s]\n",
      "train: |██▌                                      | 5/79 [00:01<00:27,  2.73it/s]\n",
      "train: |███                                      | 6/79 [00:02<00:25,  2.91it/s]\n",
      "train: |███▋                                     | 7/79 [00:02<00:23,  3.05it/s]\n",
      "train: |████▏                                    | 8/79 [00:02<00:22,  3.17it/s]\n",
      "train: |████▋                                    | 9/79 [00:02<00:21,  3.26it/s]\n",
      "train: |█████                                   | 10/79 [00:02<00:20,  3.35it/s]\n",
      "train: |█████▌                                  | 11/79 [00:03<00:19,  3.42it/s]\n",
      "train: |██████                                  | 12/79 [00:03<00:19,  3.48it/s]\n",
      "train: |██████▌                                 | 13/79 [00:03<00:18,  3.53it/s]\n",
      "train: |███████                                 | 14/79 [00:03<00:18,  3.58it/s]\n",
      "train: |███████▌                                | 15/79 [00:04<00:17,  3.62it/s]\n",
      "train: |████████                                | 16/79 [00:04<00:17,  3.66it/s]\n",
      "train: |████████▌                               | 17/79 [00:04<00:16,  3.69it/s]\n",
      "train: |█████████                               | 18/79 [00:04<00:16,  3.72it/s]\n",
      "train: |█████████▌                              | 19/79 [00:05<00:15,  3.75it/s]\n",
      "train: |██████████▏                             | 20/79 [00:05<00:15,  3.78it/s]\n",
      "train: |██████████▋                             | 21/79 [00:05<00:15,  3.80it/s]\n",
      "train: |███████████▏                            | 22/79 [00:05<00:14,  3.82it/s]\n",
      "train: |███████████▋                            | 23/79 [00:05<00:14,  3.84it/s]\n",
      "train: |████████████▏                           | 24/79 [00:06<00:14,  3.86it/s]\n",
      "train: |████████████▋                           | 25/79 [00:06<00:13,  3.87it/s]\n",
      "train: |█████████████▏                          | 26/79 [00:06<00:13,  3.89it/s]\n",
      "train: |█████████████▋                          | 27/79 [00:06<00:13,  3.90it/s]\n",
      "train: |██████████████▏                         | 28/79 [00:07<00:13,  3.92it/s]\n",
      "train: |██████████████▋                         | 29/79 [00:07<00:12,  3.93it/s]\n",
      "train: |███████████████▏                        | 30/79 [00:07<00:12,  3.94it/s]\n",
      "train: |███████████████▋                        | 31/79 [00:07<00:12,  3.95it/s]\n",
      "train: |████████████████▏                       | 32/79 [00:08<00:11,  3.97it/s]\n",
      "train: |████████████████▋                       | 33/79 [00:08<00:11,  3.97it/s]\n",
      "train: |█████████████████▏                      | 34/79 [00:08<00:11,  3.99it/s]\n",
      "train: |█████████████████▋                      | 35/79 [00:08<00:11,  3.99it/s]\n",
      "train: |██████████████████▏                     | 36/79 [00:08<00:10,  4.00it/s]\n",
      "train: |██████████████████▋                     | 37/79 [00:09<00:10,  4.01it/s]\n",
      "train: |███████████████████▏                    | 38/79 [00:09<00:10,  4.02it/s]\n",
      "train: |███████████████████▋                    | 39/79 [00:09<00:09,  4.02it/s]\n",
      "train: |████████████████████▎                   | 40/79 [00:09<00:09,  4.03it/s]\n",
      "train: |████████████████████▊                   | 41/79 [00:10<00:09,  4.04it/s]\n",
      "train: |█████████████████████▎                  | 42/79 [00:10<00:09,  4.04it/s]\n",
      "train: |█████████████████████▊                  | 43/79 [00:10<00:08,  4.05it/s]\n",
      "train: |██████████████████████▎                 | 44/79 [00:10<00:08,  4.06it/s]\n",
      "train: |██████████████████████▊                 | 45/79 [00:11<00:08,  4.06it/s]\n",
      "train: |███████████████████████▎                | 46/79 [00:11<00:08,  4.07it/s]\n",
      "train: |███████████████████████▊                | 47/79 [00:11<00:07,  4.07it/s]\n",
      "train: |████████████████████████▎               | 48/79 [00:11<00:07,  4.08it/s]\n",
      "train: |████████████████████████▊               | 49/79 [00:12<00:07,  4.08it/s]\n",
      "train: |█████████████████████████▎              | 50/79 [00:12<00:07,  4.09it/s]\n",
      "train: |█████████████████████████▊              | 51/79 [00:12<00:06,  4.09it/s]\n",
      "train: |██████████████████████████▎             | 52/79 [00:12<00:06,  4.09it/s]\n",
      "train: |██████████████████████████▊             | 53/79 [00:12<00:06,  4.10it/s]\n",
      "train: |███████████████████████████▎            | 54/79 [00:13<00:06,  4.10it/s]\n",
      "train: |███████████████████████████▊            | 55/79 [00:13<00:05,  4.10it/s]\n",
      "train: |████████████████████████████▎           | 56/79 [00:13<00:05,  4.11it/s]\n",
      "train: |████████████████████████████▊           | 57/79 [00:13<00:05,  4.11it/s]\n",
      "train: |█████████████████████████████▎          | 58/79 [00:14<00:05,  4.12it/s]\n",
      "train: |█████████████████████████████▊          | 59/79 [00:14<00:04,  4.12it/s]\n",
      "train: |██████████████████████████████▍         | 60/79 [00:14<00:04,  4.12it/s]\n",
      "train: |██████████████████████████████▉         | 61/79 [00:14<00:04,  4.12it/s]\n",
      "train: |███████████████████████████████▍        | 62/79 [00:15<00:04,  4.13it/s]\n",
      "train: |███████████████████████████████▉        | 63/79 [00:15<00:03,  4.13it/s]\n",
      "train: |████████████████████████████████▍       | 64/79 [00:15<00:03,  4.13it/s]\n",
      "train: |████████████████████████████████▉       | 65/79 [00:15<00:03,  4.14it/s]\n",
      "train: |█████████████████████████████████▍      | 66/79 [00:15<00:03,  4.14it/s]\n",
      "train: |█████████████████████████████████▉      | 67/79 [00:16<00:02,  4.14it/s]\n",
      "train: |██████████████████████████████████▍     | 68/79 [00:16<00:02,  4.14it/s]\n",
      "train: |██████████████████████████████████▉     | 69/79 [00:16<00:02,  4.15it/s]\n",
      "train: |███████████████████████████████████▍    | 70/79 [00:16<00:02,  4.15it/s]\n",
      "train: |███████████████████████████████████▉    | 71/79 [00:17<00:01,  4.15it/s]\n",
      "train: |████████████████████████████████████▍   | 72/79 [00:17<00:01,  4.15it/s]\n",
      "train: |████████████████████████████████████▉   | 73/79 [00:17<00:01,  4.15it/s]\n",
      "train: |█████████████████████████████████████▍  | 74/79 [00:17<00:01,  4.16it/s]\n",
      "train: |█████████████████████████████████████▉  | 75/79 [00:18<00:00,  4.16it/s]\n",
      "train: |██████████████████████████████████████▍ | 76/79 [00:18<00:00,  4.16it/s]\n",
      "train: |██████████████████████████████████████▉ | 77/79 [00:18<00:00,  4.16it/s]\n",
      "train: |████████████████████████████████████████| 79/79 [00:18<00:00,  4.20it/s]\n",
      "79it [00:18,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.05238460307121277, train_accuray:0.0978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "213it [00:14, 14.56it/s]\n",
      "val: |█████████████████                       | 213/500 [00:33<00:45,  6.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m     corrects\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mpred\u001b[39m.\u001b[39meq(labels\u001b[39m.\u001b[39mview_as(pred))\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     66\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 67\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     69\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     72\u001b[0m loss\u001b[39m=\u001b[39mrunning_loss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(val_dl\u001b[39m.\u001b[39mdataset)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/optim/adam.py:363\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    360\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[1;32m    362\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m    364\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss=[]\n",
    "val_loss=[]\n",
    "train_acc=[]\n",
    "val_acc=[]\n",
    "max_epoch=100\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    print('*'*30)\n",
    "    print(f'{epoch+1}/{max_epoch}')\n",
    "    \n",
    "    tq=tqdm(train_dl, ncols=80, smoothing=0, bar_format='train: {desc}|{bar}{r_bar}')\n",
    "    vtq=tqdm(val_dl, ncols=80, smoothing=0, bar_format='val: {desc}|{bar}{r_bar}')\n",
    "    \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    corrects=0\n",
    "    for i, data in tqdm(enumerate(tq)):\n",
    "        # [inputs, labels]의 목록인 data로부터 입력을 받은 후;\n",
    "        inputs, labels = data\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "       \n",
    "\n",
    "        # 변화도(Gradient) 매개변수를 0으로 만들고\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파 + 역전파 + 최적화를 한 후\n",
    "        output = net(inputs)\n",
    "        print(output)\n",
    "        loss = criterion(output, labels)\n",
    "        pred=output.argmax(1,keepdim=True)\n",
    "        corrects+=pred.eq(labels.view_as(pred)).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 통계를 출력합니다.\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    \n",
    "    loss=running_loss/len(train_dl.dataset)\n",
    "    acc=corrects/len(train_dl.dataset)\n",
    "    train_loss.append(loss)\n",
    "    train_acc.append(acc)\n",
    "    \n",
    "    print(f'train_loss: {loss}, train_accuray:{acc*100}%')\n",
    "    \n",
    "    \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    corrects=0\n",
    "    for i, data in tqdm(enumerate(vtq)):\n",
    "        inputs, labels = data\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "       \n",
    "\n",
    "        # 변화도(Gradient) 매개변수를 0으로 만들고\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파 + 역전파 + 최적화를 한 후\n",
    "        output = net(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        pred=output.argmax(1,keepdim=True)\n",
    "        corrects+=pred.eq(labels.view_as(pred)).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    \n",
    "    loss=running_loss/len(val_dl.dataset)\n",
    "    acc=corrects/len(val_dl.dataset)\n",
    "    val_loss.append(loss)\n",
    "    val_acc.append(acc)\n",
    "    \n",
    "    print(f'val_loss: {loss}, val_accuracy{acc*100}%')\n",
    "    running_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
